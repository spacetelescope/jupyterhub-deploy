#!/usr/bin/env python

import os
import subprocess
import json
import math
import random
import string
import operator
import argparse

from datetime import datetime
from dateutil.relativedelta import relativedelta
from datadog_api_client.v1 import ApiClient, Configuration
from datadog_api_client.v1.api.metrics_api import MetricsApi

"""
As written now, this script is not very extensible, and aims to collect user data for
a given week from DataDog.  User hours per week is not a direct DataDog metric, so a time
range of one week is specified for the API query.

The query response itself contains timeseries data.  It just so happens that by fetching
data for a week results in the series intervals being one hour, which is what we need.
Had the interval been different, we would have needed to do the binning ourselves.

See https://github.com/spacetelescope/jupyterhub-deploy/tree/main/doc/DATADOG_USER_METRICS.md
for more information.
"""


def get_key(key_type):
    top = os.environ.get('JUPYTERHUB_DIR')
    script = os.path.join(top, 'tools', 'get-datadog-key')
    p = subprocess.Popen(
        [script, key_type],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    out, err = p.communicate()
    return str(out).lstrip("b'").rstrip("\\n'")


def get_raw_data(args):
    os.environ['DD_SITE'] = 'datadoghq.com'
    os.environ['DD_API_KEY'] = get_key('api')
    os.environ['DD_APP_KEY'] = get_key('app')

    account = f'aws-account-name:{args["account-name"]}'
    cluster = f'eks_cluster-name:{args["cluster-name"]}'
    pod_name = 'pod_name:jupyter-*'
    query = f'max:kubernetes_state.pod.count{{{account},{cluster},{pod_name}}} by {{pod_name}}'

    end_date = f'{args["end-date"]}:23:59:59' # GMT
    to = datetime.strptime(end_date, '%Y-%m-%d:%H:%M:%S')
    _from = to + relativedelta(weeks=-1)

    configuration = Configuration()
    with ApiClient(configuration) as api_client:
        api_instance = MetricsApi(api_client)
        response = api_instance.query_metrics(
            _from=int(_from.timestamp()),
            to=int(to.timestamp()),
            query=query
        )

        return response


def parse_data(data):
    users_raw = {}
    for s in data['series']:
        user = s['tag_set'][0]
        users_raw[user] = []
        for p in s['pointlist']:
            val = p.value[1]
            if val:
                users_raw[user].append(val)

    # anonymise identifiable info
    users = {}
    for u in users_raw:
        hours = math.ceil(float(len(users_raw[u])))
        anon_user = ''.join(random.choice(string.ascii_letters) for i in range(6))
        users[anon_user] = hours

    return users


def print_report(data, end_date):
    _sorted = sorted(data.items(), key=operator.itemgetter(1), reverse=True)

    title = f'\nUser data for week ending on {end_date}'
    print(title)
    print('-'*len(title))
    print(f'\nNumber of unique users: {int(len(_sorted))}\n')
    print('Total usage hours per user (anonymised):')
    for (u, val) in _sorted:
        print(f'\t{u}\t{val}')
    print()



if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Get usage data for unique users from DataDog.  For a specified week, report number of unique users and number of hours on system per user.'
    )
    parser.add_argument(
        'end-date', type=str,
        help='end of reporting period [YYYY-MM-DD]'
    )
    parser.add_argument(
        'account-name', type=str,
        help='name of AWS account'
    )
    parser.add_argument(
        'cluster-name', type=str,
        help='name of EKS cluster'
    )
    args = parser.parse_args()
    args = vars(args)

    raw_data = get_raw_data(args)

    user_data = parse_data(raw_data)

    print_report(user_data, args['end-date'])
